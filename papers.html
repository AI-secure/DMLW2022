<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Security and Safety in Machine Learning Systems | Workshop at ICLR 2021</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Security and Safety in Machine Learning Systems" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Workshop at ICLR 2021" />
<meta property="og:description" content="Workshop at ICLR 2021" />
<meta property="og:site_name" content="Security and Safety in Machine Learning Systems" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Security and Safety in Machine Learning Systems" />
<script type="application/ld+json">
{"headline":"Security and Safety in Machine Learning Systems","url":"/papers.html","description":"Workshop at ICLR 2021","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/aml-iclr2021/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Security and Safety in Machine Learning Systems</h1>
      <h2 class="project-tagline">Workshop at ICLR 2021</h2>
      
      
      <a href="/aml-iclr2021" class="btn">Home</a>
      
      <a href="/aml-iclr2021/cfp" class="btn">Call for Papers</a>
      
      <a href="/aml-iclr2021/papers" class="btn">Accepted Papers</a>
      
      <a href="/aml-iclr2021/schedule" class="btn">Schedule</a>
      
      <a href="/aml-iclr2021/speakers" class="btn">Speakers</a>
      
      <a href="/aml-iclr2021/organizers" class="btn">Organizers</a>
      
      <a href="/aml-iclr2021/committee" class="btn">Program Committee</a>
      
      <a href="/aml-iclr2021/related" class="btn">Related Workshops</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="accepted-papers">Accepted Papers</h1>

<ul>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/0.pdf">Efficient Disruptions of Black-box Image Translation Deepfake Generation Systems</a></b> <br /> Nataniel Ruiz (Boston University); Sarah Bargal (Boston University); Stan Sclaroff (Boston University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/1.pdf">Bridging the Gap Between Adversarial Robustness and Optimization Bias</a></b> <br /> Fartash Faghri (University of Toronto); Cristina Vasconcelos (Google); David J Fleet (University of Toronto); Fabian Pedregosa (Google); Nicolas Le Roux (Google)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/2.pdf">Covariate Shift Adaptation for Adversarially Robust Classifier</a></b> <br /> Jay Nandy (National University of Singapore); Sudipan Saha (Technical University of Munich); Wynne Hsu (National University of Singapore); Mong Li Lee (National University of Singapore); Xiaoxiang Zhu (Technical University of Munich,Germany)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/3.pdf">Poisoned classifiers are not only backdoored, they are fundamentally broken</a></b> <br /> Mingjie Sun (Carnegie Mellon University); Siddhant Agarwal (Indian Institute of Technology, Kharagpur); Zico Kolter (Carnegie Mellon University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/4.pdf">Reliably fast adversarial training via latent adversarial perturbation</a></b> <br /> Geon Yeong Park (KAIST); Sang Wan Lee (KAIST)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/5.pdf">Imbalanced Gradients: A New Cause of Overestimated Adversarial Robustness</a></b> <br /> Linxi Jiang (Fudan University); Xingjun Ma (Deakin University); Zejia Weng (Fudan University); James Bailey (THE UNIVERSITY OF MELBOURNE); Yu-Gang Jiang (Fudan University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/6.pdf">SHIFT INVARIANCE CAN REDUCE ADVERSARIAL ROBUSTNESS</a></b> <br /> David Jacobs (University of Maryland, USA); Ronen Basri (Weizmann Institute of Science); Vasu Singla (University Of Maryland); Songwei Ge (University of Maryland)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/7.pdf">What is Wrong with One-Class Anomaly Detection?</a></b> <br /> JuneKyu Park (Ajou University); Jeong-Hyeon Moon (Ajou University); Namhyuk Ahn (Ajou University); Kyung-Ah Sohn (Ajou University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/8.pdf">Safe Model-based Reinforcement Learning with Robust Cross-Entropy Method</a></b> <br /> Zuxin Liu (Carnegie Mellon University); Hongyi Zhou (CMU); Baiming Chen (Tsinghua University); Sicheng Zhong (University of Toronto); DING ZHAO (Carnegie Mellon University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/9.pdf">GateNet: Bridging the gap between Binarized Neural Network and FHE evaluation</a></b> <br /> Cheng Fu (University of California, San Diego); Hanxian Huang (UC San Diego); Xinyun Chen (UC Berkeley); Jishen Zhao (University of California, San Diego)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/10.pdf">High-Robustness, Low-Transferability Fingerprinting of Neural Networks</a></b> <br /> Siyue Wang (Northeastern University); Xiao Wang (Boston University); Pin-Yu Chen (IBM Research); Pu Zhao (Northeastern University); Xue Lin (Northeastern University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/11.pdf">Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers</a></b> <br /> Kiran Karra (JHU/APL); Chace Ashcraft (JHU/APL)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/12.pdf">Non-Singular Adversarial Robustness of Neural Networks</a></b> <br /> Yu-Lin Tsai (National Chiao Tung University); Chia-Yi Hsu (National Yang Ming Chiao Tung University); Chia-Mu Yu (National Chiao Tung University); Pin-Yu Chen (IBM Research)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/13.pdf">Adversarial Examples Make Stronger Poisons</a></b> <br /> Liam Fowl (University of Maryland); Micah Goldblum (University of Maryland, College Park); Ping-yeh Chiang (University of Maryland, College Park); Jonas A. Geiping (University of Siegen); Tom Goldstein (University of Maryland, College Park)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/14.pdf">What Doesn’t Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors</a></b> <br /> Jonas A. Geiping (University of Siegen); Liam Fowl (University of Maryland); Gowthami Somepalli (University of Maryland); Micah Goldblum (University of Maryland); Michael Moeller (University of Siegen); Tom Goldstein (University of Maryland, College Park)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/15.pdf">Byzantine-Robust and Privacy-Preserving Framework for FedML</a></b> <br /> Hanieh Hashemi (University of Southern California); Yongqin Wang (University of Southern California); Chuan Guo (Facebook AI Research); Murali Annavaram (University of Southern California)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/16.pdf">Data Augmentation Can Improve Robustness</a></b> <br /> Sylvestre-Alvise Rebuffi (DeepMind); Sven Gowal (DeepMind); Dan Andrei Calian (DeepMind); Florian Stimberg (); Olivia Wiles (DeepMind); Timothy Arthur Mann (DeepMind)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/17.pdf">Baseline Pruning-Based Approach to Trojan Detection in Neural Networks</a></b> <br /> Peter Bajcsy (NIST); Michael Majurski (NIST)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/18.pdf">Extracting Hyperparameter Constraints From Code</a></b> <br /> Ingkarat Rak-amnouykit (Rensselaer Polytechnic Institute); Ana Milanova (Rensselaer Polytechnic Institute); Guillaume Baudart (Inria Paris, École normale supérieure - PSL University); Martin Hirzel (IBM Research); Julian Dolby (IBM Research)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/19.pdf">Coordinated Attacks Against Federated Learning: A Multi-Agent Reinforcement Learning Approach</a></b> <br /> Wen Shen (Tulane University); Henger Li (Tulane University); Zizhan Zheng (Tulane University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/20.pdf">Regularization Can Help Mitigate Poisoning Attacks… with the Right Hyperparameters</a></b> <br /> Javier Carnerero-Cano (Imperial College London); Luis Muñoz-González (Imperial College London); Phillippa Spencer (Defence Science and Technology Laboratory); Emil Lupu (Imperial College London)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/21.pdf">Ditto: Fair and Robust Federated Learning Through Personalization</a></b> <br /> Tian Li (Carnegie Mellon University); Shengyuan Hu (Carnegie Mellon University); Ahmad Beirami (Facebook AI); Virginia Smith (Carnegie Mellon University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/22.pdf">Low Curvature Activations Reduce Overfitting in Adversarial Training</a></b> <br /> Vasu Singla (University Of Maryland); Sahil Singla (University of Maryland); David Jacobs (University of Maryland, USA); Soheil Feizi (University of Maryland)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/23.pdf">Examining Trends in Out-of-Domain Confidence</a></b> <br /> Hamza Qadeer (University of California, Berkeley); Michael Chau (University of California, Berkeley); Eric Zhu (University of California, Berkeley); Matthew A Wright (University of California Berkeley); Richard Liaw (UC Berkeley) <br /> <a href="https://www.youtube.com/watch?v=-r9rk5Gvip4">YouTube presentation</a></li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/24.pdf">Doing More with Less: Improving Robustness using Generated Data</a></b> <br /> Sven Gowal (DeepMind); Sylvestre-Alvise Rebuffi (DeepMind); Olivia Wiles (DeepMind); Florian Stimberg (); Dan Andrei Calian (DeepMind); Timothy Arthur Mann (DeepMind)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/25.pdf">Hidden Backdoor Attack against Semantic Segmentation Models</a></b> <br /> Yiming Li (Tsinghua University); Yanjie Li (Tsinghua University); Yalei Lv (Tsinghua University); Yong Jiang (Tsinghua University); Shutao Xia (Tsinghua University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/26.pdf">$\delta$-CLUE: Diverse Sets of Explanations for Uncertainty Estimates</a></b> <br /> Dan Ley (University of Cambridge); Umang Bhatt (University of Cambridge); Adrian Weller (University of Cambridge)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/27.pdf">Safe Exploration Method for Reinforcement Learning under Existence of Disturbance</a></b> <br /> Yoshihiro Okawa (Fujitsu Laboratories Ltd.); Yusuke Kato (Keio University); Tomotake Sasaki (Fujitsu Laboratories Ltd.); Hitoshi Yanami (Fujitsu Laboratories Ltd.); Toru Namerikawa (Keio University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/28.pdf">Provable defense by denoised smoothing with learned score function</a></b> <br /> Kyungmin Lee (Agency for Defense Development)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/29.pdf">Boosting black-box adversarial attack via exploiting loss smoothness</a></b> <br /> Hoang Tran (Oak Ridge National Lab); Dan Lu (Oak Ridge National Laboratory); Guannan Zhang (Oak Ridge National Laboratory) <br /> <a href="https://www.youtube.com/watch?v=9GdQrHtccZ8">YouTube presentation</a></li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/30.pdf">PatchGuard++: Efficient Provable Attack Detection against Adversarial Patches</a></b> <br /> Chong Xiang (Princeton University); Prateek Mittal (Princeton University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/31.pdf">DEEP GRADIENT ATTACK WITH STRONG DP-SGD LOWER BOUND FOR LABEL PRIVACY</a></b> <br /> Sen Yuan (Facebook); Min Xue (Facebook); Kaikai Wang (Facebook); Milan Shen (Facebook)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/32.pdf">Measuring Adversarial Robustness using a Voronoi-Epsilon Adversary</a></b> <br /> Hyeongji Kim (Institute of Marine Research); Pekka Parviainen (University of Bergen); Ketil Malde (UIB)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/33.pdf">Accelerated Policy Evaluation with Adaptive Importance Sampling</a></b> <br /> Mengdi Xu (Carnegie Mellon University); Peide Huang (Carnegie Mellon University); Fengpei Li (Columbia University); Jiacheng Zhu (Carnegie Mellon University); ‪Xuewei (Tony) Qi (Toyota North America Research Labs); Zhiyuan Huang (Tongji University); Henry Lam (Columbia University); DING ZHAO (Carnegie Mellon University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/34.pdf">Sparse Coding Frontend for Robust Neural Networks</a></b> <br /> Can Bakiskan (University of California, Santa Barbara); Metehan Cekic (University of California, Santa Barbara); Ahmet D Sezer (University of California, Santa Barbara); Upamanyu Madhow (University of California, Santa Barbara)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/35.pdf">Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting</a></b> <br /> Xiangyu Qi (Zhejiang University); Jifeng Zhu (Tencent); Chulin Xie (University of Illinois at Urbana-Champaign); Yong Yang (Tencent)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/36.pdf">Speeding Up Neural Network Verification via Automated Algorithm Configuration</a></b> <br /> Matthias König (Leiden University); Holger Hoos (Leiden Institute of Advanced Computer Science, Leiden University); Jan Van Rijn (Leiden University) <br /> <a href="https://youtu.be/3YMAx1if2hM">YouTube presentation</a></li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/37.pdf">Incorporating Label Uncertainty in Intrinsic Robustness Measures</a></b> <br /> Xiao Zhang (University of Virginia); David Evans (University of Virginia)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/38.pdf">FIRM: Detecting Adversarial Audios by Recursive Filters with Randomization</a></b> <br /> Guanhong Tao (Purdue University); Xiaowei Chen (Baidu X-Lab); Yunhan Jia (Bytedance Inc.); Zhenyu Zhong (Baidu); Shiqing Ma (Rutgers University); Xiangyu Zhang (Purdue University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/39.pdf">On Improving Adversarial Robustness Using Proxy Distributions</a></b> <br /> Vikash Sehwag (Princeton University); Saeed Mahloujifar (Princeton University); Sihui Dai (California Institute of Technology); Tinashe Handina (Princeton University); Chong Xiang (Princeton University); Mung Chiang (Purdue University); Prateek Mittal (Princeton University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/40.pdf">Detecting Adversarial Attacks through Neural Activations</a></b> <br /> Graham Annett (Boise State University); Hoda Mehrpouyan (Boise State University); Tim Andersen (Boise State); Casey R Kennington (Boise State University); Craig Primer (Boise State University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/41.pdf">Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release</a></b> <br /> Liam Fowl (University of Maryland); Ping-yeh Chiang (University of Maryland, College Park); Micah Goldblum (University of Maryland, College Park); Jonas A. Geiping (University of Siegen); Arpit Bansal (University of Maryland - College Park); Wojciech Czaja (University of Maryland, College Park); Tom Goldstein (University of Maryland, College Park)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/42.pdf">Robustness from Perception</a></b> <br /> Saeed Mahloujifar (Princeton University); Chong Xiang (Princeton University); Vikash Sehwag (Princeton University); Sihui Dai (California Institute of Technology); Prateek Mittal (Princeton University)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/43.pdf">Mitigating Adversarial Training Instability with Batch Normalization</a></b> <br /> Arvind Sridhar (UC Berkeley); Chawin Sitawarin (UC Berkeley); David Wagner (UC Berkeley)  <br /> <a href="https://youtu.be/R2Dtnsm4M1c">YouTube presentation</a></li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/44.pdf">Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks</a></b> <br /> Dequan Wang (UC Berkeley); Evan Shelhamer (Imaginary Number); An Ju (University of California, Berkeley); David Wagner (UC Berkeley); Trevor Darrell (UC Berkeley)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/45.pdf">Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers</a></b> <br /> Francesco Croce (University of Tübingen); Matthias Hein (University of Tübingen)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/46.pdf">DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations</a></b> <br /> Eitan Borgnia (University of Maryland); Jonas A. Geiping (University of Siegen); Valeriia Cherepanova (University of Maryland); Liam Fowl (University of Maryland); Arjun Gupta (University of Maryland College Park); Amin Ghiasi (University of Maryland); Furong Huang (University of Maryland); Micah Goldblum (University of Maryland); Tom Goldstein (University of Maryland, College Park)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/47.pdf">RobustBench: a standardized adversarial robustness benchmark</a></b> <br /> Francesco Croce (University of Tübingen); Maksym Andriushchenko (EPFL); Vikash Sehwag (Princeton University); Edoardo Debenedetti (EPFL); Nicolas Flammarion (EPFL); Mung Chiang (Princeton University); Prateek Mittal (Princeton University); Matthias Hein (University of Tübingen)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/48.pdf">Simple Transparent Adversarial Examples</a></b> <br /> Jaydeep Jitendra Borkar (Savitribai Phule Pune University); Pin-Yu Chen (IBM Research)</li>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/49.pdf">Moral Scenarios for Reinforcement Learning Agents</a></b> <br /> Dan Hendrycks (UC Berkeley); Mantas Mazeika (UIUC); Andy Zou (UC Berkeley); Sahil Patel (UC Berkeley); Christine Zhu (UC Berkeley); Jesus Navarro (UC Berkeley); Bo Li (UIUC); Dawn Song (UC Berkeley); Jacob Steinhardt (UC Berkeley)</li>
</ul>


      <footer class="site-footer">
        <span class="site-footer-credits">Please contact <a href="mailto:xinyun.chen@berkeley.edu">Xinyun Chen</a> or <a href="mailto:cihangxie306@gmail.com">Cihang Xie</a> if you have any questions.<br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>
          on Sat, 08 May 2021 18:58:06.
        </span>
      </footer>
    </main>
  </body>
</html>
